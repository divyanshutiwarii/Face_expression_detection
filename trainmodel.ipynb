{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec8439a",
   "metadata": {},
   "source": [
    "## **Step 1: Setting Up the Model for Face Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e33269",
   "metadata": {},
   "source": [
    "### 1.1. Instal Required Libraries\n",
    "```python \n",
    "- torch==1.9.1\n",
    "- torchvision==0.10.1\n",
    "- numpy==1.21.0\n",
    "- pandas==1.3.0\n",
    "- Pillow==8.2.0\n",
    "- scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8453044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (3.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from flask) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\divyanshu tiwari\\.conda\\envs\\tfgpu\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebf2b4",
   "metadata": {},
   "source": [
    "### 1.2. Import Torch Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33085b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e85aa",
   "metadata": {},
   "source": [
    "### 1.3. Checking for GPU Availability\n",
    "\n",
    "Next, we'll check if a GPU is available for accelerated training. If a GPU is detected, we'll set the device to CUDA; otherwise, we'll use the CPU for computations. This step ensures that we can leverage hardware acceleration if it's accessible. Let's proceed with the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18face6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU ({torch.cuda.get_device_name(0)}) is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02c9b8",
   "metadata": {},
   "source": [
    "## **Step 2: Data Preparation**\n",
    "\n",
    "In this step, we'll focus on preparing the dataset for training. This includes loading the necessary libraries and performing tasks such as reading image data and organizing it for later use in the model. Let's move on to the code implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af3b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7e371",
   "metadata": {},
   "source": [
    "### 2.1. Setting Directory Locations\n",
    "\n",
    "Now, let's define the directory locations for the training and testing datasets. These paths will be used to access the image data for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71670669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Location\n",
    "TRAIN_DIR = 'C:/Users/divyanshu tiwari/Desktop/Divyanshu/New_Face_expression/dataset/dataset/images/images/train'\n",
    "TEST_DIR = 'C:/Users/divyanshu tiwari/Desktop/Divyanshu/New_Face_expression/dataset/dataset/images/images/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c14ce",
   "metadata": {},
   "source": [
    "### 2.2. Creating Custom Dataset and DataFrames\n",
    "\n",
    "To effectively handle our image data, we'll create a custom dataset class and corresponding data frames for both training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c5f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n",
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
    "        label = int(self.dataframe.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def create_dataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return pd.DataFrame({'image': image_paths, 'label': labels})\n",
    "\n",
    "# Create DataFrames\n",
    "train = create_dataframe(TRAIN_DIR)\n",
    "test = create_dataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9a176",
   "metadata": {},
   "source": [
    "### 2.3. Data Transformations\n",
    "\n",
    "To prepare the data for training, we'll apply a series of transformations to the images. These transformations help in standardizing the format and preparing the data for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d860469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438a9a7",
   "metadata": {},
   "source": [
    "## **Step 3: Label Encoding**\n",
    "\n",
    "In this step, we perform label encoding to convert categorical labels into numerical format. This is important for training the model as it requires numerical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a492e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create LabelEncoder instance\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels in your train and test DataFrames\n",
    "train['label'] = le.fit_transform(train['label'])\n",
    "test['label'] = le.transform(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf1ddad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Dataset: 20649\n",
      "Total Test Dataset : 14668\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Train Dataset: {len(train)}')\n",
    "print(f'Total Test Dataset : {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e807b2",
   "metadata": {},
   "source": [
    "## **Step 4: Load Data into DataLoader**\n",
    "\n",
    "Now that we have our dataset ready, we create DataLoaders. DataLoaders help in efficient handling of data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc30f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataLoader\n",
    "train_dataset = CustomDataset(dataframe=train, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(dataframe=test, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc320",
   "metadata": {},
   "source": [
    "## **Step 5: Define a Simple CNN Architecture**\n",
    "\n",
    "In this step, you have defined a simple Convolutional Neural Network (CNN) architecture for your Face Sentiment Analysis task. This architecture is designed to process grayscale images and classify them into one of seven emotion categories.\n",
    "\n",
    "The architecture comprises the following components:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   - `conv1`: 16 filters, 3x3 kernel size, ReLU activation, padding = 1.\n",
    "   - `conv2`: 32 filters, 3x3 kernel size, ReLU activation, padding = 1.\n",
    "   - `conv3`: 64 filters, 3x3 kernel size, ReLU activation, padding = 1.\n",
    "\n",
    "2. **Max Pooling Layers:**\n",
    "   - Max pooling with 2x2 kernel size and stride of 2 is applied after each convolutional layer.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - `fc1`: 512 output units, ReLU activation.\n",
    "   - `fc2`: Output layer with units equal to the number of classes (in this case, 7 for the 7 emotions).\n",
    "\n",
    "4. **Forward Pass:**\n",
    "   - The input image is passed through the convolutional layers with ReLU activation and max pooling.\n",
    "   - The output is then flattened and passed through the fully connected layers.\n",
    "\n",
    "This architecture serves as a solid foundation for your Face Sentiment Analysis model. In the next steps, you will proceed with training, evaluation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ce419f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simpler CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c71af",
   "metadata": {},
   "source": [
    "## **Step 6: Initialize the Model**\n",
    "\n",
    "In this step, you have initialized your Simple CNN model. Additionally, you have checked if a CUDA-enabled GPU is available and moved the model to the GPU for faster training.\n",
    "\n",
    "```python\n",
    "# Initialize your model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "This ensures that your model is ready for training on the available hardware resources. Next, you'll move on to the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c8aed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model = SimpleCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2654c",
   "metadata": {},
   "source": [
    "## **Step 7: Define Loss Function and Optimizer**\n",
    "\n",
    "In this step, you've defined the loss function and optimizer for your model.\n",
    "\n",
    "```python\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "The chosen loss function (`nn.CrossEntropyLoss`) is commonly used for multi-class classification tasks. The optimizer (`optim.Adam`) is an efficient optimization algorithm that adapts the learning rate during training.\n",
    "\n",
    "You're now set to begin the training process. Proceed to Step 10 for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8914583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c4a7f",
   "metadata": {},
   "source": [
    "## **Step 8: Model Evaluation and Performance Analysis**\n",
    "\n",
    "In this step, we've implemented the training loop for your Face Sentiment Analysis model. This loop will iterate through the specified number of epochs and perform the following operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4f8f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 1.7244, Training Accuracy: 0.31, Validation Loss: 1.6465, Validation Accuracy: 0.39\n",
      "Epoch [2/30], Training Loss: 1.5194, Training Accuracy: 0.44, Validation Loss: 1.5038, Validation Accuracy: 0.44\n",
      "Epoch [3/30], Training Loss: 1.4037, Training Accuracy: 0.48, Validation Loss: 1.4558, Validation Accuracy: 0.46\n",
      "Epoch [4/30], Training Loss: 1.3127, Training Accuracy: 0.51, Validation Loss: 1.3180, Validation Accuracy: 0.50\n",
      "Epoch [5/30], Training Loss: 1.2296, Training Accuracy: 0.55, Validation Loss: 1.2687, Validation Accuracy: 0.53\n",
      "Epoch [6/30], Training Loss: 1.1584, Training Accuracy: 0.57, Validation Loss: 1.2356, Validation Accuracy: 0.54\n",
      "Epoch [7/30], Training Loss: 1.0829, Training Accuracy: 0.60, Validation Loss: 1.1958, Validation Accuracy: 0.55\n",
      "Epoch [8/30], Training Loss: 1.0154, Training Accuracy: 0.63, Validation Loss: 1.1619, Validation Accuracy: 0.58\n",
      "Epoch [9/30], Training Loss: 0.9365, Training Accuracy: 0.66, Validation Loss: 1.1194, Validation Accuracy: 0.60\n",
      "Epoch [10/30], Training Loss: 0.8503, Training Accuracy: 0.69, Validation Loss: 1.1638, Validation Accuracy: 0.60\n",
      "Epoch [11/30], Training Loss: 0.7723, Training Accuracy: 0.72, Validation Loss: 1.1283, Validation Accuracy: 0.62\n",
      "Epoch [12/30], Training Loss: 0.6741, Training Accuracy: 0.76, Validation Loss: 1.1057, Validation Accuracy: 0.64\n",
      "Epoch [13/30], Training Loss: 0.5754, Training Accuracy: 0.80, Validation Loss: 1.1589, Validation Accuracy: 0.64\n",
      "Epoch [14/30], Training Loss: 0.4645, Training Accuracy: 0.84, Validation Loss: 1.2566, Validation Accuracy: 0.65\n",
      "Epoch [15/30], Training Loss: 0.3693, Training Accuracy: 0.87, Validation Loss: 1.2388, Validation Accuracy: 0.67\n",
      "Epoch [16/30], Training Loss: 0.2844, Training Accuracy: 0.91, Validation Loss: 1.4021, Validation Accuracy: 0.67\n",
      "Epoch [17/30], Training Loss: 0.2068, Training Accuracy: 0.94, Validation Loss: 1.6204, Validation Accuracy: 0.67\n",
      "Epoch [18/30], Training Loss: 0.1636, Training Accuracy: 0.95, Validation Loss: 1.6914, Validation Accuracy: 0.68\n",
      "Epoch [19/30], Training Loss: 0.1258, Training Accuracy: 0.96, Validation Loss: 2.1450, Validation Accuracy: 0.65\n",
      "Epoch [20/30], Training Loss: 0.0962, Training Accuracy: 0.97, Validation Loss: 1.8941, Validation Accuracy: 0.69\n",
      "Epoch [21/30], Training Loss: 0.0743, Training Accuracy: 0.98, Validation Loss: 2.0673, Validation Accuracy: 0.68\n",
      "Epoch [22/30], Training Loss: 0.0638, Training Accuracy: 0.99, Validation Loss: 1.9840, Validation Accuracy: 0.70\n",
      "Epoch [23/30], Training Loss: 0.0604, Training Accuracy: 0.99, Validation Loss: 2.0985, Validation Accuracy: 0.69\n",
      "Epoch [24/30], Training Loss: 0.0563, Training Accuracy: 0.99, Validation Loss: 2.2064, Validation Accuracy: 0.68\n",
      "Epoch [25/30], Training Loss: 0.0564, Training Accuracy: 0.99, Validation Loss: 2.2742, Validation Accuracy: 0.69\n",
      "Epoch [26/30], Training Loss: 0.0627, Training Accuracy: 0.98, Validation Loss: 2.4454, Validation Accuracy: 0.67\n",
      "Epoch [27/30], Training Loss: 0.0567, Training Accuracy: 0.99, Validation Loss: 2.2990, Validation Accuracy: 0.69\n",
      "Epoch [28/30], Training Loss: 0.0655, Training Accuracy: 0.98, Validation Loss: 2.3959, Validation Accuracy: 0.68\n",
      "Epoch [29/30], Training Loss: 0.0620, Training Accuracy: 0.98, Validation Loss: 2.3731, Validation Accuracy: 0.69\n",
      "Epoch [30/30], Training Loss: 0.0448, Training Accuracy: 0.99, Validation Loss: 2.4714, Validation Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have dataloaders defined as train_loader and test_loader\n",
    "# Initialize lists to store accuracies and losses\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation/Test\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:  # Assuming you have a separate validation loader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    # Append accuracies and losses to their respective lists\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Training Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Training Accuracy: {train_accuracy:.2f}, '\n",
    "          f'Validation Loss: {val_loss:.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "087aed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 7.2647, Total Accuracy: 98.95%\n"
     ]
    }
   ],
   "source": [
    "# Checking Accuracy and Running Loss\n",
    "total_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "# Print total loss and total accuracy\n",
    "print(f'Total Loss: {running_loss:.4f}, Total Accuracy: {total_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba90c8f",
   "metadata": {},
   "source": [
    "## **Final: Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d338d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is named 'SimpleCNN'\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'emotiondetector.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
